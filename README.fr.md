# Coveo DevOps Challenge

## Le d√©fi

L'un de vos coll√®gues a commenc√© un projet pour analyser les fichiers d'un bucket S3, en extraire des informations et
calculer le co√ªt de stockage. Le projet n'est pas termin√© et il a d√ª partir pour des raisons personnelles.
Votre gestionnaire vous demande de terminer le projet et de le pr√©parer pour la mise en production.

## Sp√©cifications

L‚Äôoutil doit se pr√©senter sous forme d‚Äôune commande shell qui permet d‚Äôobtenir des informations sur l‚Äôensemble des ressources [S3](https://aws.amazon.com/documentation/s3/) d‚Äôun compte Amazon.

- L'outil doit fonctionner sous Linux, OSX et Windows.
- Il doit √™tre simple √† installer et √† utiliser.
- Id√©alement, l'outil ne devrait pas n√©cessiter l'installation de libraries et / ou outils externes pour √™tre fonctionnel.
- Le temps est de l'argent, nous ne pouvons pas attendre des heures pour obtenir des r√©sultats. La solution devrait nous retourner des r√©ponses en quelques secondes (ou en quelques minutes si vous tenez √† tester notre patience :-).

### Configuration d'environnement

Pour chaque bucket:
  - Nom
  - Date de cr√©ation
  - Nombre de fichiers
  - Taille totale des fichiers
  - Date de mise-√†-jour de l'objet le plus r√©cent
  - Et le plus important de tous, **combien √ßa co√ªte...**

Votre coll√®gue a d√©j√† commenc√© la t√¢che et il en a accompli une bonne partie. Vous trouverez son code dans cette branche.
**Bien que ce code soit d√©j√† fonctionnel, vous devrez porter attention aux quelques TODO's qui restent √† faire.**

## Afin de vous pr√©parer pour l'entretien:

- Assurez-vous de pouvoir ex√©cuter le code et de comprendre ce qui s'y passe.
- Passez en revue le code et prenez des notes sur ce que vous aimeriez am√©liorer ou changer. Supposons que ce code soit
sur le point d'√™tre mis en production et que vous deviez planifier les prochaines versions. Quelles seraient vos priorit√©s
pour la premi√®re version, la deuxi√®me version, etc. Cela nous aidera √† concentrer la discussion sur ce qui est important en premier.
- Assurez-vous d'avoir r√©gl√© tous les TODO's laiss√©s dans le code.
- Assurez-vous d'avoir un environnement qui vous permet de placer un point d'arr√™t et de d√©boguer le code √©tape par √©tape.
Peu importe l'application que vous utilisez pour le faire, mais assurez-vous d'√™tre √† l'aise avec le d√©bogage dans l'environnement
que vous choisissez avant l'entretien, car il y aura des bugs üòâ.
- Ayez un √©diteur ou un IDE pr√™t √† coder pendant l'entretien.
- Ayez Git install√©.

Votre coll√®gue qui a commenc√© ce projet n'a pas suivi nos normes habituelles, vous devriez donc avoir quelque chose √† dire √† ce sujet.
Si vous voulez aller plus loin, vous pouvez am√©liorer son travail avant l'entretien, mais il est plus important d'√™tre capable
de commenter ce que vous pensez √™tre probl√©matique et pourquoi vous pensez que cela pourrait √™tre am√©lior√©. Nous ne recherchons pas
une solution parfaite, nous sommes plus int√©ress√©s par votre processus de r√©flexion et la fa√ßon dont vous aborderiez le probl√®me.

Nous nous attendons √† ce que vous compreniez le projet dans son ensemble et que vous ayez un avis technique sur celui-ci. Nous
comprenons cependant que vous ne soyez pas 100% familiaris√© avec AWS. C'est normal et nous ne vous demandons pas d'apprendre tout avant l'entretien.

## Lancer le projet

1. Tout d'abord, vous devez cr√©er un compte AWS. Un compte gratuit peut √™tre cr√©√©. 
2. Cr√©ez un bucket S3 et t√©l√©chargez-y quelques fichiers. Gardez √† l'esprit que vous pourriez √™tre factur√© si vous d√©passez
   les [conditions de gratuit√©](https://aws.amazon.com/free/?all-free-tier.sort-by=item.additionalFields.SortRank&all-free-tier.sort-order=asc&awsf.Free%20Tier%20Types=*all&awsf.Free%20Tier%20Categories=*all&all-free-tier.q=S3&all-free-tier.q_operator=AND)
   (5 GiB au moment de la r√©daction).
3. Pour ex√©cuter le projet, vous aurez besoin de Python 3.8 ou d'une version plus r√©cente et de [Poetry](https://python-poetry.org/docs/#installation)
4. Ex√©cutez `poetry install`
5. Ex√©cutez `poetry run python ./main.py`

## Pendant l'entretien

Soyez pr√™t pour une revue de code et une discussion sur le projet. Gardez en t√™te que nous pourrions vous demander d'ex√©cuter
le code dans un environnement diff√©rent du v√¥tre avec un grand nombre de fichiers.

## Conseils finaux

Amusez-vous. Gardez en t√™te qu'il est rare que des candidats saisissent √† l'avance tous les probl√®mes potentiels √† √©viter.
Nous ne recherchons pas la perfection, vous serez plut√¥t √©valu√© sur votre adaptabilit√© et votre capacit√© √† r√©soudre les probl√®mes vari√©s
auxquels vous pourriez √™tre confront√©.

Pendant l'entretien, traitez les interviewers comme des coll√®gues. N'h√©sitez pas √† demander de l'aide comme vous le feriez
normalement dans le cadre de votre travail. Ils sont l√† pour vous aider, pas pour vous pi√©ger, et nous voulons que vous r√©ussissiez.
Leur principal objectif est de trouver en vous un futur coll√®gue avec qui ils appr√©cieront travailler.

## Notice sur l'utilisation de l'IA g√©n√©rative (GitHub Copilot)

GitHub Copilot a √©t√© utilis√© dans le cadre du processus de r√©alisation de cette √©valuation. N√©anmoins, des recherches approfondies et des r√©visions manuelles ont √©t√© effectu√©es pour obtenir un r√©sultat optimal et respecter la port√©e du travail. Dans le paysage actuel du d√©veloppement logiciel, les comp√©tences d'un bon ing√©nieur comprennent d√©sormais la capacit√© √† utiliser correctement les Mod√®les de Langage de Grande Taille (LLM) en mode agent. GitHub Copilot et d'autres outils d'IA similaires sont devenus des multiplicateurs de productivit√© essentiels qui permettent aux d√©veloppeurs de se concentrer sur l'architecture de haut niveau, la r√©solution de probl√®mes et la qualit√© du code, tout en automatisant les t√¢ches r√©p√©titives et en fournissant des suggestions de code intelligentes.

## Nouvelles fonctionnalit√©s et am√©liorations

Cette version am√©lior√©e inclut les am√©liorations suivantes:

### Docker Compose avec LocalStack

Pour le d√©veloppement local et les tests sans frais AWS:

1. Copiez le mod√®le d'environnement: `cp .env.template .env`
2. D√©marrez LocalStack: `docker compose up -d`
3. Ex√©cutez le script: `poetry run python -m src.main`
4. Arr√™tez LocalStack: `docker compose down`

### Fonctionnalit√©s am√©lior√©es

- **Op√©rations Async**: Utilise aioboto3 pour une performance am√©lior√©e avec un grand nombre de buckets
- **Pagination Appropri√©e**: G√®re efficacement les buckets avec des millions d'objets
- **Calcul des Co√ªts**: 
  - Tarification en temps r√©el depuis l'API AWS Pricing
  - Co√ªts mensuels estim√©s bas√©s sur le stockage actuel
- **Limitation de D√©bit**: Mode de retry adaptatif avec tentatives maximales configurables
- **Gestion d'Erreurs**: Gestion gracieuse de divers sc√©narios d'erreur AWS
- **Tests Complets**: Tests unitaires et tests e2e

### Am√©liorations futures/limitations
- Les co√ªts r√©els pourraient √™tre √©valu√©s avec Cost Explorer, ce qui serait moins cher et plus fiable que l'√©num√©ration d'objets, incluant des param√®tres tels que les co√ªts d'appels API.
- Ceci assume les prix de liste, ne tient pas compte des Accords de Tarification Priv√©s

### Consid√©rations de co√ªt et performance

**Co√ªts API Par Op√©ration:**
- Lister les buckets: ~$0.005 par 1,000 op√©rations de bucket
- Lister les objets: ~$0.0004 par 1,000 requ√™tes
- API Pricing: Niveau gratuit disponible

**Exemples de co√ªts:**
- 1M d'objets sur 10 buckets: ~$400 en op√©rations LIST
- 1,000 buckets dans le compte: ~$5 en listage de buckets

**Performance:**
- Petits buckets (<1K objets): Secondes
- Grands buckets (>1M objets): Minutes
- Multiples grands buckets: Peut prendre des heures
- Utilise un listage r√©cursif par pr√©fixe avec profondeur configurable pour une performance optimale sur les structures hi√©rarchiques

‚ö†Ô∏è **ATTENTION**: Ex√©cuter contre tous les buckets dans un compte avec de nombreux grands buckets peut entra√Æner des co√ªts API significatifs. Toujours tester avec des buckets sp√©cifiques d'abord.

## Instructions d'usage

### Ex√©cution du script (depuis la racine du d√©p√¥t)

```bash
# Analyser tous les buckets
poetry run python -m src.main

# Analyser un bucket sp√©cifique
poetry run python -m src.main mon-nom-de-bucket

# Format de sortie JSON
poetry run python -m src.main mon-nom-de-bucket --json

# Utilisation avec Docker Compose (LocalStack)
docker compose up -d
poetry run python -m src.main
docker compose down
```

### Variables de configuration

Configurez les identifiants AWS et param√®tres dans `.env`:

```bash
# Copiez le mod√®le et √©ditez
cp .env.template .env
# √âditez .env avec vos identifiants AWS
```

#### Variables de configuration de performance

- `MAX_CONCURRENT_BUCKETS`: Limite le traitement concurrent des buckets (optionnel, aucune limite par d√©faut)
- `MAX_RECURSION_DEPTH`: Contr√¥le la profondeur de listage r√©cursif par pr√©fixe (d√©faut: 4)
  - Des valeurs plus √©lev√©es am√©liorent l'efficacit√© pour les structures de dossiers profond√©ment imbriqu√©es
  - Des valeurs plus faibles r√©duisent l'utilisation m√©moire et pr√©viennent le d√©bordement de pile pour les structures extr√™mement profondes
  - √Ä la profondeur maximale, bascule vers un listage √† plat sans pr√©fixes
- `S3_PREFIX_DELIMITER`: Caract√®re utilis√© comme s√©parateur de r√©pertoire dans les cl√©s d'objets S3 (d√©faut: "/")
  - Alternatives courantes: "-", "_", "|" pour diff√©rentes conventions de nommage

### Tests

```bash
# Installer les d√©pendances
poetry install

# Ex√©cuter les tests unitaires
poetry run pytest tests/unit/ -v --cov

# Ex√©cuter les tests e2e (n√©cessite LocalStack ou AWS)
docker compose up -d  # Pour LocalStack
poetry run pytest tests/e2e/ -v

# Ex√©cuter tous les tests
poetry run pytest -v
```

### D√©bogage des tests E2E

Pour d√©boguer les tests E2E, vous pouvez utiliser le d√©bogueur `debugpy` en d√©finissant la variable d'environnement `E2E_DEBUG=1` :

```bash
# Ex√©cuter les tests E2E avec le d√©bogueur
E2E_DEBUG=1 poetry run pytest tests/e2e/test_e2e.py::TestE2ECliOutput::test_cli_with_specific_bucket -s

# D√©boguer un cas de test sp√©cifique
E2E_DEBUG=1 poetry run pytest tests/e2e/test_e2e.py::TestE2ECliOutput::test_cli_with_nonexistent_bucket -s
```

## Permissions AWS IAM requises

Pour une fonctionnalit√© compl√®te, les identifiants AWS n√©cessitent ces permissions:

```json
{
  "Version": "2012-10-17",
  "Statement": [
    {
      "Effect": "Allow",
      "Action": [
        "s3:ListAllMyBuckets",
        "s3:ListBucket",
        "s3:GetBucketLocation"
      ],
      "Resource": "*"
    },
    {
      "Effect": "Allow",
      "Action": [
        "pricing:GetProducts"
      ],
      "Resource": "*"
    }
  ]
}
```

Note: Si un seul bucket doit √™tre list√©, les permissions peuvent √™tre r√©duites en cons√©quence.

## GitHub Actions

Deux workflows sont inclus:

1. **Tests**: S'ex√©cute sur push/PR avec LocalStack
2. **Ex√©cution contre S3 r√©el**: Workflow manuel pour tester contre AWS r√©el (n√©cessite configuration des secrets)

## Outillage de s√©curit√©

Ce r√©pertoire utilise les fonctionnalit√©s GitHub Advanced Security:
- **CodeQL**: Analyse de s√©curit√© automatis√©e
- **Dependabot**: Surveillance des vuln√©rabilit√©s des d√©pendances  
- **Secret Scanning**: Pr√©vient l'exposition des identifiants

Ces outils d√©montrent les pratiques DevSecOps modernes pour le d√©veloppement logiciel s√©curis√©.
